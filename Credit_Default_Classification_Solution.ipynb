{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Classification Modeling\n",
    "The goal of this week's assessment is to **find the model which best predicts whether or not a person will default on their bank loan**. In doing so, we want to utilize all of the different tools we have learned over the course: data cleaning, EDA, feature engineering/transformation, feature selection, hyperparameter tuning, and model \n",
    "evaluation. \n",
    " Data Source: https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients\n",
    "\n",
    "\n",
    "#### Data Set Information:\n",
    "\n",
    "This research aimed at the case of customers default payments in Taiwan and compares the predictive accuracy of probability of default among six data mining methods. **From the perspective of risk management, the result of predictive accuracy of the estimated probability of default will be more valuable than the binary result of classification - credible or not credible clients.** Because the real probability of default is unknown, this study presented the novel Sorting Smoothing Method to estimate the real probability of default. \n",
    "\n",
    "With the real probability of default as the response variable (Y), and the predictive probability of default as the independent variable (X), the simple linear regression result (Y = A + BX) shows that the forecasting model produced by artificial neural network has the highest coefficient of determination; its regression intercept (A) is close to zero, and regression coefficient (B) to one. Therefore, among the six data mining techniques, artificial neural network is the only one that can accurately estimate the real probability of default. \n",
    "\n",
    "- NT is the abbreviation for New Taiwain. \n",
    "\n",
    "\n",
    "#### Attribute Information:\n",
    "\n",
    "This research employed a binary variable, default payment (Yes = 1, No = 0), as the response variable. This study reviewed the literature and used the following 23 variables as explanatory variables: \n",
    "\n",
    "- X1: Amount of the given credit (NT dollar): it includes both the individual consumer credit and his/her family (supplementary) credit. \n",
    "- X2: Gender (1 = male; 2 = female). \n",
    "- X3: Education (1 = graduate school; 2 = university; 3 = high school; 4 = others). \n",
    "- X4: Marital status (1 = married; 2 = single; 3 = others). \n",
    "- X5: Age (year). \n",
    "- X6 - X11: History of past payment. We tracked the past monthly payment records (from April to September, 2005) as follows: \n",
    "    - X6 = the repayment status in September, 2005; \n",
    "    - X7 = the repayment status in August, 2005; . . .;\n",
    "    - etc...\n",
    "    - X11 = the repayment status in April, 2005. \n",
    "    - The measurement scale for the repayment status is: -1 = pay duly; 1 = payment delay for one month; 2 = payment delay for two months; . . .; 8 = payment delay for eight months; 9 = payment delay for nine months and above. \n",
    "- X12-X17: Amount of bill statement (NT dollar). \n",
    "    - X12 = amount of bill statement in September, 2005;\n",
    "    - etc...\n",
    "    - X13 = amount of bill statement in August, 2005; . . .; \n",
    "    - X17 = amount of bill statement in April, 2005. \n",
    "- X18-X23: Amount of previous payment (NT dollar). \n",
    "    - X18 = amount paid in September, 2005; \n",
    "    - X19 = amount paid in August, 2005; . . .;\n",
    "    - etc...\n",
    "    - X23 = amount paid in April, 2005. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "You will fit three different models (KNN, Logistic Regression, and Decision Tree Classifier) to predict credit card defaults and use gridsearch to find the best hyperparameters for those models. Then you will compare the performance of those three models on a test set to find the best one.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process/Expectations\n",
    "\n",
    "- You will be working in pairs for this assessment\n",
    "\n",
    "### Please have ONE notebook and be prepared to explain how you worked in your pair.\n",
    "\n",
    "1. Clean up your data set so that you can perform an EDA. \n",
    "    - This includes handling null values, categorical variables, removing unimportant columns, and removing outliers.\n",
    "2. Perform EDA to identify opportunities to create new features.\n",
    "    - [Great Example of EDA for classification](https://www.kaggle.com/stephaniestallworth/titanic-eda-classification-end-to-end) \n",
    "    - [Using Pairplots with Classification](https://towardsdatascience.com/visualizing-data-with-pair-plots-in-python-f228cf529166)\n",
    "3. Engineer new features. \n",
    "    - Create polynomial and/or interaction features. \n",
    "    - Additionaly, you must also create **at least 2 new features** that are not interactions or polynomial transformations. \n",
    "        - *For example, you can create a new dummy variable that based on the value of a continuous variable (billamount6 >2000) or take the average of some past amounts.*\n",
    "4. Perform some feature selection. \n",
    "    \n",
    "5. You must fit **three** models to your data and tune **at least 1 hyperparameter** per model. \n",
    "6. Using the F-1 Score, evaluate how well your models perform and identify your best model.\n",
    "7. Using information from your EDA process and your model(s) output provide insight as to which borrowers are more likely to deafult\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import set_config\n",
    "set_config(print_changed_only=False)\n",
    "\n",
    "\n",
    "# import sklearn model.s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('training_data.csv' , index_col=0) #header= 18382"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>X10</th>\n",
       "      <th>X11</th>\n",
       "      <th>X12</th>\n",
       "      <th>X13</th>\n",
       "      <th>X14</th>\n",
       "      <th>X15</th>\n",
       "      <th>X16</th>\n",
       "      <th>X17</th>\n",
       "      <th>X18</th>\n",
       "      <th>X19</th>\n",
       "      <th>X20</th>\n",
       "      <th>X21</th>\n",
       "      <th>X22</th>\n",
       "      <th>X23</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28835</th>\n",
       "      <td>220000</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>222598</td>\n",
       "      <td>222168</td>\n",
       "      <td>217900</td>\n",
       "      <td>221193</td>\n",
       "      <td>181859</td>\n",
       "      <td>184605</td>\n",
       "      <td>10000</td>\n",
       "      <td>8018</td>\n",
       "      <td>10121</td>\n",
       "      <td>6006</td>\n",
       "      <td>10987</td>\n",
       "      <td>143779</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25329</th>\n",
       "      <td>200000</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>29</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>326</td>\n",
       "      <td>326</td>\n",
       "      <td>326</td>\n",
       "      <td>326</td>\n",
       "      <td>326</td>\n",
       "      <td>326</td>\n",
       "      <td>326</td>\n",
       "      <td>326</td>\n",
       "      <td>326</td>\n",
       "      <td>326</td>\n",
       "      <td>326</td>\n",
       "      <td>326</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18894</th>\n",
       "      <td>180000</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>690</th>\n",
       "      <td>80000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>51372</td>\n",
       "      <td>51872</td>\n",
       "      <td>47593</td>\n",
       "      <td>43882</td>\n",
       "      <td>42256</td>\n",
       "      <td>42527</td>\n",
       "      <td>1853</td>\n",
       "      <td>1700</td>\n",
       "      <td>1522</td>\n",
       "      <td>1548</td>\n",
       "      <td>1488</td>\n",
       "      <td>1500</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6239</th>\n",
       "      <td>10000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8257</td>\n",
       "      <td>7995</td>\n",
       "      <td>4878</td>\n",
       "      <td>5444</td>\n",
       "      <td>2639</td>\n",
       "      <td>2697</td>\n",
       "      <td>2000</td>\n",
       "      <td>1100</td>\n",
       "      <td>600</td>\n",
       "      <td>300</td>\n",
       "      <td>300</td>\n",
       "      <td>1000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           X1 X2 X3 X4  X5  X6  X7  X8  X9 X10 X11     X12     X13     X14  \\\n",
       "28835  220000  2  1  2  36   0   0   0   0   0   0  222598  222168  217900   \n",
       "25329  200000  2  3  2  29  -1  -1  -1  -1  -1  -1     326     326     326   \n",
       "18894  180000  2  1  2  27  -2  -2  -2  -2  -2  -2       0       0       0   \n",
       "690     80000  1  2  2  32   0   0   0   0   0   0   51372   51872   47593   \n",
       "6239    10000  1  2  2  27   0   0   0   0   0   0    8257    7995    4878   \n",
       "\n",
       "          X15     X16     X17    X18   X19    X20   X21    X22     X23  Y  \n",
       "28835  221193  181859  184605  10000  8018  10121  6006  10987  143779  1  \n",
       "25329     326     326     326    326   326    326   326    326     326  0  \n",
       "18894       0       0       0      0     0      0     0      0       0  0  \n",
       "690     43882   42256   42527   1853  1700   1522  1548   1488    1500  0  \n",
       "6239     5444    2639    2697   2000  1100    600   300    300    1000  1  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['ID'], inplace = True) # Extra label headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df.columns:\n",
    "    df[column] = pd.to_numeric(df[column])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(labels='2198', inplace=True) # outlier in X1  >800000\n",
    "df[\"X3\"].replace([0, 5, 6], 4, inplace = True) # 0,5,6 not described in data description\n",
    "df.drop(labels = ['23092'], inplace = True) # X1 Outlier in category 4 of X3\n",
    "df[\"X4\"].replace(0, 3, inplace = True) # 0 not describe in data description\n",
    "df['X6'].replace([-2,-1], 0, inplace = True)\n",
    "df['X7'].replace([-2,-1], 0, inplace = True)\n",
    "df['X8'].replace([-2,-1], 0, inplace = True)\n",
    "df['X9'].replace([-2,-1], 0, inplace = True)\n",
    "df['X10'].replace([-2,-1], 0, inplace = True)\n",
    "df['X11'].replace([-2,-1], 0, inplace = True)\n",
    "df.drop(labels= ['25732', '5297'], inplace = True) # X14 Outlier >800000\n",
    "df.drop(labels= ['20893'] ,inplace = True) #X16 Outlier >800000\n",
    "df.drop(labels= ['291','26098'], inplace = True) # X17 Outlier < -200000\n",
    "df.drop(labels=['28717','12331'], inplace = True) # X18 outlier >500000\n",
    "df.drop(labels=['28004','14514'], inplace = True) #X19 > 500000\n",
    "df.drop(labels=['27441'], inplace = True) #X20  outlier > 500000\n",
    "df.drop(labels=['24687'], inplace = True) # X21 outlier > 500000\n",
    "df.drop(labels=['14512','507'], inplace = True) # X23 outlier >500000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_feats =['X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X7', 'X8', 'X9', 'X10', 'X11',\n",
    "       'X12', 'X13', 'X14', 'X15', 'X16', 'X17', 'X18', 'X19', 'X20', 'X21',\n",
    "       'X22', 'X23']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = df[orig_feats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. EDA === see [here](http://localhost:8888/notebooks/Pre%20EDA.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['avg_pay_status'] = round(((df['X6'] + df['X7']+df['X8']+df['X9']+df['X10']+df['X11'])/6),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7376323183515687\n",
      "0.3757530027923717\n"
     ]
    }
   ],
   "source": [
    "print(df['avg_pay_status'].corr(df['X6'])) \n",
    "print(df['avg_pay_status'].corr(df['Y']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "pay_status_X = ['X6', 'X7', 'X8', 'X9', 'X10', 'X11']\n",
    "# df.drop(labels = pay_status_X, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>Y</td>        <th>  R-squared:         </th> <td>   0.171</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.171</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   774.7</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sun, 08 Nov 2020</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>19:55:57</td>     <th>  Log-Likelihood:    </th> <td> -10104.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td> 22484</td>      <th>  AIC:               </th> <td>2.022e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td> 22477</td>      <th>  BIC:               </th> <td>2.028e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     6</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>    0.1367</td> <td>    0.003</td> <td>   48.178</td> <td> 0.000</td> <td>    0.131</td> <td>    0.142</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>X6</th>        <td>    0.1625</td> <td>    0.005</td> <td>   34.309</td> <td> 0.000</td> <td>    0.153</td> <td>    0.172</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>X7</th>        <td>    0.0174</td> <td>    0.005</td> <td>    3.440</td> <td> 0.001</td> <td>    0.008</td> <td>    0.027</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>X8</th>        <td>    0.0293</td> <td>    0.005</td> <td>    5.791</td> <td> 0.000</td> <td>    0.019</td> <td>    0.039</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>X9</th>        <td>    0.0065</td> <td>    0.006</td> <td>    1.133</td> <td> 0.257</td> <td>   -0.005</td> <td>    0.018</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>X10</th>       <td>    0.0192</td> <td>    0.006</td> <td>    3.039</td> <td> 0.002</td> <td>    0.007</td> <td>    0.032</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>X11</th>       <td>    0.0353</td> <td>    0.005</td> <td>    6.632</td> <td> 0.000</td> <td>    0.025</td> <td>    0.046</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>3620.567</td> <th>  Durbin-Watson:     </th> <td>   1.997</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>5639.165</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td> 1.158</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td> 3.812</td>  <th>  Cond. No.          </th> <td>    5.35</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      Y   R-squared:                       0.171\n",
       "Model:                            OLS   Adj. R-squared:                  0.171\n",
       "Method:                 Least Squares   F-statistic:                     774.7\n",
       "Date:                Sun, 08 Nov 2020   Prob (F-statistic):               0.00\n",
       "Time:                        19:55:57   Log-Likelihood:                -10104.\n",
       "No. Observations:               22484   AIC:                         2.022e+04\n",
       "Df Residuals:                   22477   BIC:                         2.028e+04\n",
       "Df Model:                           6                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept      0.1367      0.003     48.178      0.000       0.131       0.142\n",
       "X6             0.1625      0.005     34.309      0.000       0.153       0.172\n",
       "X7             0.0174      0.005      3.440      0.001       0.008       0.027\n",
       "X8             0.0293      0.005      5.791      0.000       0.019       0.039\n",
       "X9             0.0065      0.006      1.133      0.257      -0.005       0.018\n",
       "X10            0.0192      0.006      3.039      0.002       0.007       0.032\n",
       "X11            0.0353      0.005      6.632      0.000       0.025       0.046\n",
       "==============================================================================\n",
       "Omnibus:                     3620.567   Durbin-Watson:                   1.997\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             5639.165\n",
       "Skew:                           1.158   Prob(JB):                         0.00\n",
       "Kurtosis:                       3.812   Cond. No.                         5.35\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "f = 'Y~X6+X7+X8+X9+X10+X11'\n",
    "model = ols(formula=f, data=df).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>Y</td>        <th>  R-squared:         </th> <td>   0.141</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.141</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   3696.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sun, 08 Nov 2020</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>19:55:59</td>     <th>  Log-Likelihood:    </th> <td> -10505.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td> 22484</td>      <th>  AIC:               </th> <td>2.101e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td> 22482</td>      <th>  BIC:               </th> <td>2.103e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "         <td></td>           <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>      <td>    0.1504</td> <td>    0.003</td> <td>   52.930</td> <td> 0.000</td> <td>    0.145</td> <td>    0.156</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>avg_pay_status</th> <td>    0.2574</td> <td>    0.004</td> <td>   60.796</td> <td> 0.000</td> <td>    0.249</td> <td>    0.266</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>3613.520</td> <th>  Durbin-Watson:     </th> <td>   1.991</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>5610.038</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td> 1.197</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td> 3.511</td>  <th>  Cond. No.          </th> <td>    1.84</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      Y   R-squared:                       0.141\n",
       "Model:                            OLS   Adj. R-squared:                  0.141\n",
       "Method:                 Least Squares   F-statistic:                     3696.\n",
       "Date:                Sun, 08 Nov 2020   Prob (F-statistic):               0.00\n",
       "Time:                        19:55:59   Log-Likelihood:                -10505.\n",
       "No. Observations:               22484   AIC:                         2.101e+04\n",
       "Df Residuals:                   22482   BIC:                         2.103e+04\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==================================================================================\n",
       "                     coef    std err          t      P>|t|      [0.025      0.975]\n",
       "----------------------------------------------------------------------------------\n",
       "Intercept          0.1504      0.003     52.930      0.000       0.145       0.156\n",
       "avg_pay_status     0.2574      0.004     60.796      0.000       0.249       0.266\n",
       "==============================================================================\n",
       "Omnibus:                     3613.520   Durbin-Watson:                   1.991\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             5610.038\n",
       "Skew:                           1.197   Prob(JB):                         0.00\n",
       "Kurtosis:                       3.511   Cond. No.                         1.84\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = 'Y~avg_pay_status'\n",
    "model2 = ols(formula=f, data=df).fit()\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Include either the avg or all the other vars in the model not both**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['avg_billamt'] = (df['X12'] + df['X13'] + df['X14'] + df['X15'] + df['X16'] + df['X17'])/6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>Y</td>        <th>  R-squared:         </th> <td>   0.001</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   2.491</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sun, 08 Nov 2020</td> <th>  Prob (F-statistic):</th>  <td>0.0207</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>19:56:04</td>     <th>  Log-Likelihood:    </th> <td> -12209.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td> 22484</td>      <th>  AIC:               </th> <td>2.443e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td> 22477</td>      <th>  BIC:               </th> <td>2.449e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     6</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>    0.2273</td> <td>    0.003</td> <td>   66.541</td> <td> 0.000</td> <td>    0.221</td> <td>    0.234</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>X12</th>       <td>-3.946e-07</td> <td> 1.28e-07</td> <td>   -3.080</td> <td> 0.002</td> <td>-6.46e-07</td> <td>-1.43e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>X13</th>       <td> 2.896e-07</td> <td> 1.64e-07</td> <td>    1.768</td> <td> 0.077</td> <td>-3.15e-08</td> <td> 6.11e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>X14</th>       <td>-5.016e-08</td> <td> 1.55e-07</td> <td>   -0.324</td> <td> 0.746</td> <td>-3.54e-07</td> <td> 2.54e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>X15</th>       <td>-6.968e-08</td> <td> 1.65e-07</td> <td>   -0.423</td> <td> 0.672</td> <td>-3.93e-07</td> <td> 2.53e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>X16</th>       <td> 5.186e-08</td> <td> 1.86e-07</td> <td>    0.279</td> <td> 0.780</td> <td>-3.12e-07</td> <td> 4.16e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>X17</th>       <td> 1.428e-07</td> <td> 1.46e-07</td> <td>    0.976</td> <td> 0.329</td> <td>-1.44e-07</td> <td>  4.3e-07</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>4048.081</td> <th>  Durbin-Watson:     </th> <td>   1.989</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>6638.820</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td> 1.326</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td> 2.761</td>  <th>  Cond. No.          </th> <td>2.35e+05</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 2.35e+05. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      Y   R-squared:                       0.001\n",
       "Model:                            OLS   Adj. R-squared:                  0.000\n",
       "Method:                 Least Squares   F-statistic:                     2.491\n",
       "Date:                Sun, 08 Nov 2020   Prob (F-statistic):             0.0207\n",
       "Time:                        19:56:04   Log-Likelihood:                -12209.\n",
       "No. Observations:               22484   AIC:                         2.443e+04\n",
       "Df Residuals:                   22477   BIC:                         2.449e+04\n",
       "Df Model:                           6                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept      0.2273      0.003     66.541      0.000       0.221       0.234\n",
       "X12        -3.946e-07   1.28e-07     -3.080      0.002   -6.46e-07   -1.43e-07\n",
       "X13         2.896e-07   1.64e-07      1.768      0.077   -3.15e-08    6.11e-07\n",
       "X14        -5.016e-08   1.55e-07     -0.324      0.746   -3.54e-07    2.54e-07\n",
       "X15        -6.968e-08   1.65e-07     -0.423      0.672   -3.93e-07    2.53e-07\n",
       "X16         5.186e-08   1.86e-07      0.279      0.780   -3.12e-07    4.16e-07\n",
       "X17         1.428e-07   1.46e-07      0.976      0.329   -1.44e-07     4.3e-07\n",
       "==============================================================================\n",
       "Omnibus:                     4048.081   Durbin-Watson:                   1.989\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             6638.820\n",
       "Skew:                           1.326   Prob(JB):                         0.00\n",
       "Kurtosis:                       2.761   Cond. No.                     2.35e+05\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 2.35e+05. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = 'Y~X12+X13+X14+X15+X16+X17'\n",
    "model = ols(formula=f, data=df).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>Y</td>        <th>  R-squared:         </th> <td>   0.000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   2.635</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sun, 08 Nov 2020</td> <th>  Prob (F-statistic):</th>  <td> 0.105</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>19:57:11</td>     <th>  Log-Likelihood:    </th> <td> -12215.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td> 22484</td>      <th>  AIC:               </th> <td>2.443e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td> 22482</td>      <th>  BIC:               </th> <td>2.445e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "       <td></td>          <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>   <td>    0.2267</td> <td>    0.003</td> <td>   66.515</td> <td> 0.000</td> <td>    0.220</td> <td>    0.233</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>avg_billamt</th> <td>-7.134e-08</td> <td>  4.4e-08</td> <td>   -1.623</td> <td> 0.105</td> <td>-1.57e-07</td> <td> 1.48e-08</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>4053.112</td> <th>  Durbin-Watson:     </th> <td>   1.989</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>6651.727</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td> 1.327</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td> 2.762</td>  <th>  Cond. No.          </th> <td>9.52e+04</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 9.52e+04. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      Y   R-squared:                       0.000\n",
       "Model:                            OLS   Adj. R-squared:                  0.000\n",
       "Method:                 Least Squares   F-statistic:                     2.635\n",
       "Date:                Sun, 08 Nov 2020   Prob (F-statistic):              0.105\n",
       "Time:                        19:57:11   Log-Likelihood:                -12215.\n",
       "No. Observations:               22484   AIC:                         2.443e+04\n",
       "Df Residuals:                   22482   BIC:                         2.445e+04\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "===============================================================================\n",
       "                  coef    std err          t      P>|t|      [0.025      0.975]\n",
       "-------------------------------------------------------------------------------\n",
       "Intercept       0.2267      0.003     66.515      0.000       0.220       0.233\n",
       "avg_billamt -7.134e-08    4.4e-08     -1.623      0.105   -1.57e-07    1.48e-08\n",
       "==============================================================================\n",
       "Omnibus:                     4053.112   Durbin-Watson:                   1.989\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             6651.727\n",
       "Skew:                           1.327   Prob(JB):                         0.00\n",
       "Kurtosis:                       2.762   Cond. No.                     9.52e+04\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 9.52e+04. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = 'Y~avg_billamt'\n",
    "model = ols(formula=f, data=df).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.010824748891872444\n",
      "count     22484.000000\n",
      "mean      44936.195954\n",
      "std       63213.254434\n",
      "min      -43253.833333\n",
      "25%        4723.791667\n",
      "50%       20877.333333\n",
      "75%       57022.750000\n",
      "max      577585.000000\n",
      "Name: avg_billamt, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(df['avg_billamt'].corr(df['Y']))\n",
    "print(df['avg_billamt'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18894    0.0\n",
       "28407    0.0\n",
       "17391    0.0\n",
       "25400    0.0\n",
       "8109     0.0\n",
       "        ... \n",
       "9818     0.0\n",
       "25532    0.0\n",
       "19       0.0\n",
       "19210    0.0\n",
       "17431    0.0\n",
       "Name: avg_billamt, Length: 808, dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['avg_billamt'] <=0]['avg_billamt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22484, 26)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df.loc[(df['avg_billamt'] > 57000)]['avg_billamt'].count()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binning the average bill amount to 4 groups in a new variable of df['billamt_status'].\n",
    "conditions = [\n",
    "    df['avg_billamt'] <= 0,\n",
    "    (df['avg_billamt'] > 0) & (df['avg_billamt'] <= 5000),\n",
    "     (df['avg_billamt'] > 5000) & (df['avg_billamt'] <= 21000),\n",
    "     (df['avg_billamt'] > 21000) & (df['avg_billamt'] <= 57000),\n",
    "     df['avg_billamt'] > 57000\n",
    "     ]\n",
    "\n",
    "choices = [\n",
    "    0,\n",
    "    1,\n",
    "    2,\n",
    "    3,\n",
    "    4\n",
    "]\n",
    "df['billamt_status'] = np.select(conditions, choices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    5623\n",
       "3    5593\n",
       "2    5530\n",
       "1    4930\n",
       "0     808\n",
       "Name: billamt_status, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['billamt_status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.024686904353862216"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['billamt_status'].corr(df['Y'])\n",
    "# has a better corr value than the avg bill amount var, with the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7091817791755448"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['billamt_status'].corr(df['avg_billamt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>Y</td>        <th>  R-squared:         </th> <td>   0.001</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.001</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   13.71</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sun, 08 Nov 2020</td> <th>  Prob (F-statistic):</th> <td>0.000214</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>20:29:36</td>     <th>  Log-Likelihood:    </th> <td> -12210.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td> 22484</td>      <th>  AIC:               </th> <td>2.442e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td> 22482</td>      <th>  BIC:               </th> <td>2.444e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "         <td></td>           <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>      <td>    0.2449</td> <td>    0.006</td> <td>   38.269</td> <td> 0.000</td> <td>    0.232</td> <td>    0.257</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>billamt_status</th> <td>   -0.0087</td> <td>    0.002</td> <td>   -3.703</td> <td> 0.000</td> <td>   -0.013</td> <td>   -0.004</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>4049.035</td> <th>  Durbin-Watson:     </th> <td>   1.989</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>6642.093</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td> 1.326</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td> 2.762</td>  <th>  Cond. No.          </th> <td>    6.99</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      Y   R-squared:                       0.001\n",
       "Model:                            OLS   Adj. R-squared:                  0.001\n",
       "Method:                 Least Squares   F-statistic:                     13.71\n",
       "Date:                Sun, 08 Nov 2020   Prob (F-statistic):           0.000214\n",
       "Time:                        20:29:36   Log-Likelihood:                -12210.\n",
       "No. Observations:               22484   AIC:                         2.442e+04\n",
       "Df Residuals:                   22482   BIC:                         2.444e+04\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==================================================================================\n",
       "                     coef    std err          t      P>|t|      [0.025      0.975]\n",
       "----------------------------------------------------------------------------------\n",
       "Intercept          0.2449      0.006     38.269      0.000       0.232       0.257\n",
       "billamt_status    -0.0087      0.002     -3.703      0.000      -0.013      -0.004\n",
       "==============================================================================\n",
       "Omnibus:                     4049.035   Durbin-Watson:                   1.989\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             6642.093\n",
       "Skew:                           1.326   Prob(JB):                         0.00\n",
       "Kurtosis:                       2.762   Cond. No.                         6.99\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = 'Y~billamt_status'\n",
    "model = ols(formula=f, data=df).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lhams/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fa4720696d8>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.boxplot('X1', by= 'billamt_status', figsize = (7,7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On average the highest credit statement bill amount has the  highest credit limit balances. However, as the credit statement bill amount \n",
    "O value has a greater than 700000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X7', 'X8', 'X9', 'X10', 'X11',\n",
       "       'X12', 'X13', 'X14', 'X15', 'X16', 'X17', 'X18', 'X19', 'X20', 'X21',\n",
       "       'X22', 'X23', 'Y', 'avg_pay_status', 'avg_billamt', 'billamt_status'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['bal_change'] = (df['X12']- df['X17'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>Y</td>        <th>  R-squared:         </th> <td>   0.000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   11.10</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sun, 08 Nov 2020</td> <th>  Prob (F-statistic):</th> <td>0.000864</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>20:30:15</td>     <th>  Log-Likelihood:    </th> <td> -12211.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td> 22484</td>      <th>  AIC:               </th> <td>2.443e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td> 22482</td>      <th>  BIC:               </th> <td>2.444e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "       <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>  <td>    0.2262</td> <td>    0.003</td> <td>   78.290</td> <td> 0.000</td> <td>    0.221</td> <td>    0.232</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bal_change</th> <td>-2.112e-07</td> <td> 6.34e-08</td> <td>   -3.332</td> <td> 0.001</td> <td>-3.35e-07</td> <td>-8.69e-08</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>4049.802</td> <th>  Durbin-Watson:     </th> <td>   1.989</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>6643.526</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td> 1.326</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td> 2.761</td>  <th>  Cond. No.          </th> <td>4.74e+04</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 4.74e+04. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      Y   R-squared:                       0.000\n",
       "Model:                            OLS   Adj. R-squared:                  0.000\n",
       "Method:                 Least Squares   F-statistic:                     11.10\n",
       "Date:                Sun, 08 Nov 2020   Prob (F-statistic):           0.000864\n",
       "Time:                        20:30:15   Log-Likelihood:                -12211.\n",
       "No. Observations:               22484   AIC:                         2.443e+04\n",
       "Df Residuals:                   22482   BIC:                         2.444e+04\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept      0.2262      0.003     78.290      0.000       0.221       0.232\n",
       "bal_change -2.112e-07   6.34e-08     -3.332      0.001   -3.35e-07   -8.69e-08\n",
       "==============================================================================\n",
       "Omnibus:                     4049.802   Durbin-Watson:                   1.989\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             6643.526\n",
       "Skew:                           1.326   Prob(JB):                         0.00\n",
       "Kurtosis:                       2.761   Cond. No.                     4.74e+04\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 4.74e+04. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = 'Y~bal_change'\n",
    "model = ols(formula=f, data=df).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     22484.000000\n",
       "mean      12523.041096\n",
       "std       43834.849193\n",
       "min     -394967.000000\n",
       "25%       -2874.000000\n",
       "50%         945.000000\n",
       "75%       19710.750000\n",
       "max      708323.000000\n",
       "Name: bal_change, dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['bal_change'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[(df['bal_change'] <= 0),'bal_change'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.04300797324970015"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['bal_change'].corr(df['Y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X7', 'X8', 'X9', 'X10', 'X11',\n",
       "       'X12', 'X13', 'X14', 'X15', 'X16', 'X17', 'X18', 'X19', 'X20', 'X21',\n",
       "       'X22', 'X23', 'Y', 'avg_pay_status', 'avg_billamt', 'billamt_status',\n",
       "       'bal_change'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "billamt_X = ['X12', 'X13', 'X14', 'X15', 'X16', 'X17']\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns = billamt_X, axis = 1, inplace= True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X7', 'X8', 'X9', 'X10', 'X11',\n",
       "       'X18', 'X19', 'X20', 'X21', 'X22', 'X23', 'Y', 'avg_pay_status',\n",
       "       'avg_billamt', 'billamt_status', 'bal_change'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy1 = pd.get_dummies(df['X2'],drop_first= True) # dummy for sex 1 is male and 2 is female\n",
    "dummy2 = pd.get_dummies(df['X3'], drop_first = True) #dummy for education 1 for grad school, 2 for University, 3 for High School and 4 for others\n",
    "dummy3 = pd.get_dummies(df['X4'], drop_first=True ) #dummy for marriage 1 = married; 2 = single; 3 = others\n",
    "dummy4 = pd.get_dummies(df['billamt_status'], drop_first = True) #for avg bill amount of vars from X12 to X17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_feats = ['X2', 'X3', 'X4', 'avg_billamt', 'billamt_status'] #avg_billamt used to build billamt status\n",
    "df.drop(columns = dummy_feats, axis = 1, inplace= True )\n",
    "df.drop(columns = 'avg_pay_status', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['X1', 'X5', 'X6', 'X7', 'X8', 'X9', 'X10', 'X11', 'X18', 'X19', 'X20',\n",
       "       'X21', 'X22', 'X23', 'Y', 'bal_change'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22484, 16)\n",
      "(22484, 1)\n",
      "(22484, 3)\n",
      "(22484, 2)\n",
      "(22484, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Int64Index([1, 2, 3, 4], dtype='int64')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.shape)\n",
    "print(dummy1.shape)\n",
    "print(dummy2.shape)\n",
    "print(dummy3.shape)\n",
    "print(dummy4.shape)\n",
    "dummy4.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### polynomial features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['28835', '25329', '18894', '690', '6239', '14912', '9405', '12899',\n",
       "       '6495', '11632',\n",
       "       ...\n",
       "       '6229', '11463', '2715', '18588', '6528', '16247', '2693', '8076',\n",
       "       '20213', '7624'],\n",
       "      dtype='object', length=22484)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1 = df[['X1', 'X5', 'X6', 'X7', 'X8', 'X9', 'X10', 'X11', 'X18', 'X19', 'X20',\n",
    "       'X21', 'X22', 'X23']]\n",
    "data1.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiating polynomial features object\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "\n",
    "# applying polynomial feature object to data\n",
    "poly_data = poly.fit_transform(data1)\n",
    "\n",
    "#assigning feature names from main dataframe\n",
    "poly_columns = poly.get_feature_names(data1.columns)\n",
    "\n",
    "#converting array to dataframe\n",
    "df_poly = pd.DataFrame(poly_data, columns = poly_columns, index=data1.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>X10</th>\n",
       "      <th>X11</th>\n",
       "      <th>X18</th>\n",
       "      <th>X19</th>\n",
       "      <th>X20</th>\n",
       "      <th>X21</th>\n",
       "      <th>X22</th>\n",
       "      <th>X23</th>\n",
       "      <th>X1^2</th>\n",
       "      <th>X1 X5</th>\n",
       "      <th>X1 X6</th>\n",
       "      <th>X1 X7</th>\n",
       "      <th>X1 X8</th>\n",
       "      <th>X1 X9</th>\n",
       "      <th>X1 X10</th>\n",
       "      <th>X1 X11</th>\n",
       "      <th>X1 X18</th>\n",
       "      <th>X1 X19</th>\n",
       "      <th>X1 X20</th>\n",
       "      <th>X1 X21</th>\n",
       "      <th>X1 X22</th>\n",
       "      <th>X1 X23</th>\n",
       "      <th>X5^2</th>\n",
       "      <th>X5 X6</th>\n",
       "      <th>X5 X7</th>\n",
       "      <th>X5 X8</th>\n",
       "      <th>X5 X9</th>\n",
       "      <th>X5 X10</th>\n",
       "      <th>X5 X11</th>\n",
       "      <th>X5 X18</th>\n",
       "      <th>X5 X19</th>\n",
       "      <th>X5 X20</th>\n",
       "      <th>X5 X21</th>\n",
       "      <th>X5 X22</th>\n",
       "      <th>X5 X23</th>\n",
       "      <th>X6^2</th>\n",
       "      <th>X6 X7</th>\n",
       "      <th>X6 X8</th>\n",
       "      <th>X6 X9</th>\n",
       "      <th>X6 X10</th>\n",
       "      <th>X6 X11</th>\n",
       "      <th>X6 X18</th>\n",
       "      <th>X6 X19</th>\n",
       "      <th>X6 X20</th>\n",
       "      <th>X6 X21</th>\n",
       "      <th>X6 X22</th>\n",
       "      <th>X6 X23</th>\n",
       "      <th>X7^2</th>\n",
       "      <th>X7 X8</th>\n",
       "      <th>X7 X9</th>\n",
       "      <th>X7 X10</th>\n",
       "      <th>X7 X11</th>\n",
       "      <th>X7 X18</th>\n",
       "      <th>X7 X19</th>\n",
       "      <th>X7 X20</th>\n",
       "      <th>X7 X21</th>\n",
       "      <th>X7 X22</th>\n",
       "      <th>X7 X23</th>\n",
       "      <th>X8^2</th>\n",
       "      <th>X8 X9</th>\n",
       "      <th>X8 X10</th>\n",
       "      <th>X8 X11</th>\n",
       "      <th>X8 X18</th>\n",
       "      <th>X8 X19</th>\n",
       "      <th>X8 X20</th>\n",
       "      <th>X8 X21</th>\n",
       "      <th>X8 X22</th>\n",
       "      <th>X8 X23</th>\n",
       "      <th>X9^2</th>\n",
       "      <th>X9 X10</th>\n",
       "      <th>X9 X11</th>\n",
       "      <th>X9 X18</th>\n",
       "      <th>X9 X19</th>\n",
       "      <th>X9 X20</th>\n",
       "      <th>X9 X21</th>\n",
       "      <th>X9 X22</th>\n",
       "      <th>X9 X23</th>\n",
       "      <th>X10^2</th>\n",
       "      <th>X10 X11</th>\n",
       "      <th>X10 X18</th>\n",
       "      <th>X10 X19</th>\n",
       "      <th>X10 X20</th>\n",
       "      <th>X10 X21</th>\n",
       "      <th>X10 X22</th>\n",
       "      <th>X10 X23</th>\n",
       "      <th>X11^2</th>\n",
       "      <th>X11 X18</th>\n",
       "      <th>X11 X19</th>\n",
       "      <th>X11 X20</th>\n",
       "      <th>X11 X21</th>\n",
       "      <th>X11 X22</th>\n",
       "      <th>X11 X23</th>\n",
       "      <th>X18^2</th>\n",
       "      <th>X18 X19</th>\n",
       "      <th>X18 X20</th>\n",
       "      <th>X18 X21</th>\n",
       "      <th>X18 X22</th>\n",
       "      <th>X18 X23</th>\n",
       "      <th>X19^2</th>\n",
       "      <th>X19 X20</th>\n",
       "      <th>X19 X21</th>\n",
       "      <th>X19 X22</th>\n",
       "      <th>X19 X23</th>\n",
       "      <th>X20^2</th>\n",
       "      <th>X20 X21</th>\n",
       "      <th>X20 X22</th>\n",
       "      <th>X20 X23</th>\n",
       "      <th>X21^2</th>\n",
       "      <th>X21 X22</th>\n",
       "      <th>X21 X23</th>\n",
       "      <th>X22^2</th>\n",
       "      <th>X22 X23</th>\n",
       "      <th>X23^2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28835</th>\n",
       "      <td>220000.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>8018.0</td>\n",
       "      <td>10121.0</td>\n",
       "      <td>6006.0</td>\n",
       "      <td>10987.0</td>\n",
       "      <td>143779.0</td>\n",
       "      <td>4.840000e+10</td>\n",
       "      <td>7920000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.200000e+09</td>\n",
       "      <td>1.763960e+09</td>\n",
       "      <td>2.226620e+09</td>\n",
       "      <td>1.321320e+09</td>\n",
       "      <td>2.417140e+09</td>\n",
       "      <td>3.163138e+10</td>\n",
       "      <td>1296.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>360000.0</td>\n",
       "      <td>288648.0</td>\n",
       "      <td>364356.0</td>\n",
       "      <td>216216.0</td>\n",
       "      <td>395532.0</td>\n",
       "      <td>5176044.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100000000.0</td>\n",
       "      <td>80180000.0</td>\n",
       "      <td>101210000.0</td>\n",
       "      <td>60060000.0</td>\n",
       "      <td>109870000.0</td>\n",
       "      <td>1.437790e+09</td>\n",
       "      <td>64288324.0</td>\n",
       "      <td>81150178.0</td>\n",
       "      <td>48156108.0</td>\n",
       "      <td>88093766.0</td>\n",
       "      <td>1.152820e+09</td>\n",
       "      <td>102434641.0</td>\n",
       "      <td>60786726.0</td>\n",
       "      <td>111199427.0</td>\n",
       "      <td>1.455187e+09</td>\n",
       "      <td>36072036.0</td>\n",
       "      <td>65987922.0</td>\n",
       "      <td>863536674.0</td>\n",
       "      <td>120714169.0</td>\n",
       "      <td>1.579700e+09</td>\n",
       "      <td>2.067240e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25329</th>\n",
       "      <td>200000.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>326.0</td>\n",
       "      <td>326.0</td>\n",
       "      <td>326.0</td>\n",
       "      <td>326.0</td>\n",
       "      <td>326.0</td>\n",
       "      <td>326.0</td>\n",
       "      <td>4.000000e+10</td>\n",
       "      <td>5800000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.520000e+07</td>\n",
       "      <td>6.520000e+07</td>\n",
       "      <td>6.520000e+07</td>\n",
       "      <td>6.520000e+07</td>\n",
       "      <td>6.520000e+07</td>\n",
       "      <td>6.520000e+07</td>\n",
       "      <td>841.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9454.0</td>\n",
       "      <td>9454.0</td>\n",
       "      <td>9454.0</td>\n",
       "      <td>9454.0</td>\n",
       "      <td>9454.0</td>\n",
       "      <td>9454.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>106276.0</td>\n",
       "      <td>106276.0</td>\n",
       "      <td>106276.0</td>\n",
       "      <td>106276.0</td>\n",
       "      <td>106276.0</td>\n",
       "      <td>1.062760e+05</td>\n",
       "      <td>106276.0</td>\n",
       "      <td>106276.0</td>\n",
       "      <td>106276.0</td>\n",
       "      <td>106276.0</td>\n",
       "      <td>1.062760e+05</td>\n",
       "      <td>106276.0</td>\n",
       "      <td>106276.0</td>\n",
       "      <td>106276.0</td>\n",
       "      <td>1.062760e+05</td>\n",
       "      <td>106276.0</td>\n",
       "      <td>106276.0</td>\n",
       "      <td>106276.0</td>\n",
       "      <td>106276.0</td>\n",
       "      <td>1.062760e+05</td>\n",
       "      <td>1.062760e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18894</th>\n",
       "      <td>180000.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.240000e+10</td>\n",
       "      <td>4860000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>729.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>690</th>\n",
       "      <td>80000.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1853.0</td>\n",
       "      <td>1700.0</td>\n",
       "      <td>1522.0</td>\n",
       "      <td>1548.0</td>\n",
       "      <td>1488.0</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>6.400000e+09</td>\n",
       "      <td>2560000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.482400e+08</td>\n",
       "      <td>1.360000e+08</td>\n",
       "      <td>1.217600e+08</td>\n",
       "      <td>1.238400e+08</td>\n",
       "      <td>1.190400e+08</td>\n",
       "      <td>1.200000e+08</td>\n",
       "      <td>1024.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>59296.0</td>\n",
       "      <td>54400.0</td>\n",
       "      <td>48704.0</td>\n",
       "      <td>49536.0</td>\n",
       "      <td>47616.0</td>\n",
       "      <td>48000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3433609.0</td>\n",
       "      <td>3150100.0</td>\n",
       "      <td>2820266.0</td>\n",
       "      <td>2868444.0</td>\n",
       "      <td>2757264.0</td>\n",
       "      <td>2.779500e+06</td>\n",
       "      <td>2890000.0</td>\n",
       "      <td>2587400.0</td>\n",
       "      <td>2631600.0</td>\n",
       "      <td>2529600.0</td>\n",
       "      <td>2.550000e+06</td>\n",
       "      <td>2316484.0</td>\n",
       "      <td>2356056.0</td>\n",
       "      <td>2264736.0</td>\n",
       "      <td>2.283000e+06</td>\n",
       "      <td>2396304.0</td>\n",
       "      <td>2303424.0</td>\n",
       "      <td>2322000.0</td>\n",
       "      <td>2214144.0</td>\n",
       "      <td>2.232000e+06</td>\n",
       "      <td>2.250000e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6239</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>600.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1.000000e+08</td>\n",
       "      <td>270000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.000000e+07</td>\n",
       "      <td>1.100000e+07</td>\n",
       "      <td>6.000000e+06</td>\n",
       "      <td>3.000000e+06</td>\n",
       "      <td>3.000000e+06</td>\n",
       "      <td>1.000000e+07</td>\n",
       "      <td>729.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>54000.0</td>\n",
       "      <td>29700.0</td>\n",
       "      <td>16200.0</td>\n",
       "      <td>8100.0</td>\n",
       "      <td>8100.0</td>\n",
       "      <td>27000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4000000.0</td>\n",
       "      <td>2200000.0</td>\n",
       "      <td>1200000.0</td>\n",
       "      <td>600000.0</td>\n",
       "      <td>600000.0</td>\n",
       "      <td>2.000000e+06</td>\n",
       "      <td>1210000.0</td>\n",
       "      <td>660000.0</td>\n",
       "      <td>330000.0</td>\n",
       "      <td>330000.0</td>\n",
       "      <td>1.100000e+06</td>\n",
       "      <td>360000.0</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>6.000000e+05</td>\n",
       "      <td>90000.0</td>\n",
       "      <td>90000.0</td>\n",
       "      <td>300000.0</td>\n",
       "      <td>90000.0</td>\n",
       "      <td>3.000000e+05</td>\n",
       "      <td>1.000000e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             X1    X5   X6   X7   X8   X9  X10  X11      X18     X19      X20  \\\n",
       "28835  220000.0  36.0  0.0  0.0  0.0  0.0  0.0  0.0  10000.0  8018.0  10121.0   \n",
       "25329  200000.0  29.0  0.0  0.0  0.0  0.0  0.0  0.0    326.0   326.0    326.0   \n",
       "18894  180000.0  27.0  0.0  0.0  0.0  0.0  0.0  0.0      0.0     0.0      0.0   \n",
       "690     80000.0  32.0  0.0  0.0  0.0  0.0  0.0  0.0   1853.0  1700.0   1522.0   \n",
       "6239    10000.0  27.0  0.0  0.0  0.0  0.0  0.0  0.0   2000.0  1100.0    600.0   \n",
       "\n",
       "          X21      X22       X23          X1^2      X1 X5  X1 X6  X1 X7  \\\n",
       "28835  6006.0  10987.0  143779.0  4.840000e+10  7920000.0    0.0    0.0   \n",
       "25329   326.0    326.0     326.0  4.000000e+10  5800000.0    0.0    0.0   \n",
       "18894     0.0      0.0       0.0  3.240000e+10  4860000.0    0.0    0.0   \n",
       "690    1548.0   1488.0    1500.0  6.400000e+09  2560000.0    0.0    0.0   \n",
       "6239    300.0    300.0    1000.0  1.000000e+08   270000.0    0.0    0.0   \n",
       "\n",
       "       X1 X8  X1 X9  X1 X10  X1 X11        X1 X18        X1 X19        X1 X20  \\\n",
       "28835    0.0    0.0     0.0     0.0  2.200000e+09  1.763960e+09  2.226620e+09   \n",
       "25329    0.0    0.0     0.0     0.0  6.520000e+07  6.520000e+07  6.520000e+07   \n",
       "18894    0.0    0.0     0.0     0.0  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "690      0.0    0.0     0.0     0.0  1.482400e+08  1.360000e+08  1.217600e+08   \n",
       "6239     0.0    0.0     0.0     0.0  2.000000e+07  1.100000e+07  6.000000e+06   \n",
       "\n",
       "             X1 X21        X1 X22        X1 X23    X5^2  X5 X6  X5 X7  X5 X8  \\\n",
       "28835  1.321320e+09  2.417140e+09  3.163138e+10  1296.0    0.0    0.0    0.0   \n",
       "25329  6.520000e+07  6.520000e+07  6.520000e+07   841.0    0.0    0.0    0.0   \n",
       "18894  0.000000e+00  0.000000e+00  0.000000e+00   729.0    0.0    0.0    0.0   \n",
       "690    1.238400e+08  1.190400e+08  1.200000e+08  1024.0    0.0    0.0    0.0   \n",
       "6239   3.000000e+06  3.000000e+06  1.000000e+07   729.0    0.0    0.0    0.0   \n",
       "\n",
       "       X5 X9  X5 X10  X5 X11    X5 X18    X5 X19    X5 X20    X5 X21  \\\n",
       "28835    0.0     0.0     0.0  360000.0  288648.0  364356.0  216216.0   \n",
       "25329    0.0     0.0     0.0    9454.0    9454.0    9454.0    9454.0   \n",
       "18894    0.0     0.0     0.0       0.0       0.0       0.0       0.0   \n",
       "690      0.0     0.0     0.0   59296.0   54400.0   48704.0   49536.0   \n",
       "6239     0.0     0.0     0.0   54000.0   29700.0   16200.0    8100.0   \n",
       "\n",
       "         X5 X22     X5 X23  X6^2  X6 X7  X6 X8  X6 X9  X6 X10  X6 X11  X6 X18  \\\n",
       "28835  395532.0  5176044.0   0.0    0.0    0.0    0.0     0.0     0.0     0.0   \n",
       "25329    9454.0     9454.0   0.0    0.0    0.0    0.0     0.0     0.0     0.0   \n",
       "18894       0.0        0.0   0.0    0.0    0.0    0.0     0.0     0.0     0.0   \n",
       "690     47616.0    48000.0   0.0    0.0    0.0    0.0     0.0     0.0     0.0   \n",
       "6239     8100.0    27000.0   0.0    0.0    0.0    0.0     0.0     0.0     0.0   \n",
       "\n",
       "       X6 X19  X6 X20  X6 X21  X6 X22  X6 X23  X7^2  X7 X8  X7 X9  X7 X10  \\\n",
       "28835     0.0     0.0     0.0     0.0     0.0   0.0    0.0    0.0     0.0   \n",
       "25329     0.0     0.0     0.0     0.0     0.0   0.0    0.0    0.0     0.0   \n",
       "18894     0.0     0.0     0.0     0.0     0.0   0.0    0.0    0.0     0.0   \n",
       "690       0.0     0.0     0.0     0.0     0.0   0.0    0.0    0.0     0.0   \n",
       "6239      0.0     0.0     0.0     0.0     0.0   0.0    0.0    0.0     0.0   \n",
       "\n",
       "       X7 X11  X7 X18  X7 X19  X7 X20  X7 X21  X7 X22  X7 X23  X8^2  X8 X9  \\\n",
       "28835     0.0     0.0     0.0     0.0     0.0     0.0     0.0   0.0    0.0   \n",
       "25329     0.0     0.0     0.0     0.0     0.0     0.0     0.0   0.0    0.0   \n",
       "18894     0.0     0.0     0.0     0.0     0.0     0.0     0.0   0.0    0.0   \n",
       "690       0.0     0.0     0.0     0.0     0.0     0.0     0.0   0.0    0.0   \n",
       "6239      0.0     0.0     0.0     0.0     0.0     0.0     0.0   0.0    0.0   \n",
       "\n",
       "       X8 X10  X8 X11  X8 X18  X8 X19  X8 X20  X8 X21  X8 X22  X8 X23  X9^2  \\\n",
       "28835     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   0.0   \n",
       "25329     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   0.0   \n",
       "18894     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   0.0   \n",
       "690       0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   0.0   \n",
       "6239      0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   0.0   \n",
       "\n",
       "       X9 X10  X9 X11  X9 X18  X9 X19  X9 X20  X9 X21  X9 X22  X9 X23  X10^2  \\\n",
       "28835     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0    0.0   \n",
       "25329     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0    0.0   \n",
       "18894     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0    0.0   \n",
       "690       0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0    0.0   \n",
       "6239      0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0    0.0   \n",
       "\n",
       "       X10 X11  X10 X18  X10 X19  X10 X20  X10 X21  X10 X22  X10 X23  X11^2  \\\n",
       "28835      0.0      0.0      0.0      0.0      0.0      0.0      0.0    0.0   \n",
       "25329      0.0      0.0      0.0      0.0      0.0      0.0      0.0    0.0   \n",
       "18894      0.0      0.0      0.0      0.0      0.0      0.0      0.0    0.0   \n",
       "690        0.0      0.0      0.0      0.0      0.0      0.0      0.0    0.0   \n",
       "6239       0.0      0.0      0.0      0.0      0.0      0.0      0.0    0.0   \n",
       "\n",
       "       X11 X18  X11 X19  X11 X20  X11 X21  X11 X22  X11 X23        X18^2  \\\n",
       "28835      0.0      0.0      0.0      0.0      0.0      0.0  100000000.0   \n",
       "25329      0.0      0.0      0.0      0.0      0.0      0.0     106276.0   \n",
       "18894      0.0      0.0      0.0      0.0      0.0      0.0          0.0   \n",
       "690        0.0      0.0      0.0      0.0      0.0      0.0    3433609.0   \n",
       "6239       0.0      0.0      0.0      0.0      0.0      0.0    4000000.0   \n",
       "\n",
       "          X18 X19      X18 X20     X18 X21      X18 X22       X18 X23  \\\n",
       "28835  80180000.0  101210000.0  60060000.0  109870000.0  1.437790e+09   \n",
       "25329    106276.0     106276.0    106276.0     106276.0  1.062760e+05   \n",
       "18894         0.0          0.0         0.0          0.0  0.000000e+00   \n",
       "690     3150100.0    2820266.0   2868444.0    2757264.0  2.779500e+06   \n",
       "6239    2200000.0    1200000.0    600000.0     600000.0  2.000000e+06   \n",
       "\n",
       "            X19^2     X19 X20     X19 X21     X19 X22       X19 X23  \\\n",
       "28835  64288324.0  81150178.0  48156108.0  88093766.0  1.152820e+09   \n",
       "25329    106276.0    106276.0    106276.0    106276.0  1.062760e+05   \n",
       "18894         0.0         0.0         0.0         0.0  0.000000e+00   \n",
       "690     2890000.0   2587400.0   2631600.0   2529600.0  2.550000e+06   \n",
       "6239    1210000.0    660000.0    330000.0    330000.0  1.100000e+06   \n",
       "\n",
       "             X20^2     X20 X21      X20 X22       X20 X23       X21^2  \\\n",
       "28835  102434641.0  60786726.0  111199427.0  1.455187e+09  36072036.0   \n",
       "25329     106276.0    106276.0     106276.0  1.062760e+05    106276.0   \n",
       "18894          0.0         0.0          0.0  0.000000e+00         0.0   \n",
       "690      2316484.0   2356056.0    2264736.0  2.283000e+06   2396304.0   \n",
       "6239      360000.0    180000.0     180000.0  6.000000e+05     90000.0   \n",
       "\n",
       "          X21 X22      X21 X23        X22^2       X22 X23         X23^2  \n",
       "28835  65987922.0  863536674.0  120714169.0  1.579700e+09  2.067240e+10  \n",
       "25329    106276.0     106276.0     106276.0  1.062760e+05  1.062760e+05  \n",
       "18894         0.0          0.0          0.0  0.000000e+00  0.000000e+00  \n",
       "690     2303424.0    2322000.0    2214144.0  2.232000e+06  2.250000e+06  \n",
       "6239      90000.0     300000.0      90000.0  3.000000e+05  1.000000e+06  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_poly.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_poly.drop(columns =['X1', 'X5', 'X6', 'X7', 'X8', 'X9', 'X10', 'X11', 'X18', 'X19', 'X20',\n",
    "       'X21', 'X22', 'X23'], axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['X1', 'X5', 'X6', 'X7', 'X8', 'X9', 'X10', 'X11', 'X18', 'X19', 'X20',\n",
       "       'X21', 'X22', 'X23', 'Y', 'bal_change'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poly_features = list(df_poly.columns)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22484, 16)\n",
      "(22484, 1)\n",
      "(22484, 3)\n",
      "(22484, 2)\n",
      "(22484, 4)\n",
      "(22484, 105)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape) #has Y\n",
    "print(dummy1.shape) # no Y\n",
    "print(dummy2.shape) # no Y\n",
    "print(dummy3.shape) # no Y\n",
    "print(dummy4.shape) # no Y\n",
    "print(df_poly.shape) # no Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22484, 131)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.concat([df, df_poly, dummy1, dummy2, dummy3, dummy4 ], axis = 1)\n",
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data to be used in the models\n",
    "# Create matrix of features\n",
    "X = df1.drop('Y', axis = 1) # grabs everything else but 'default column'\n",
    "\n",
    "\n",
    "# Create target variable\n",
    "y = df1['Y'] # y is the column we're trying to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28835    1\n",
       "25329    0\n",
       "18894    0\n",
       "690      0\n",
       "6239     1\n",
       "Name: Y, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set - Features:  (15738, 130) Target:  (15738,)\n",
      "Test set - Features:  (6746, 130) Target:  (6746,)\n"
     ]
    }
   ],
   "source": [
    "#performing train-test split on main dataframe\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=150, test_size=0.3)\n",
    "\n",
    "#checking the shape of the training set and test set\n",
    "print(\"Training set - Features: \", X_train.shape, \"Target: \", y_train.shape,)\n",
    "print(\"Test set - Features: \", X_test.shape, \"Target: \",y_test.shape,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.775384\n",
      "1    0.224616\n",
      "Name: Y, dtype: float64\n",
      "0    0.77898\n",
      "1    0.22102\n",
      "Name: Y, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(y_train.value_counts(normalize = True))\n",
    "print(y_test.value_counts(normalize = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select K Best = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['X1', 'X6', 'X7', 'X8', 'X9', 'X10', 'X11', 'X1^2', 'X1 X5', 'X1 X6',\n",
       "       'X1 X7', 'X1 X8', 'X1 X9', 'X1 X10', 'X1 X11', 'X5 X6', 'X5 X7',\n",
       "       'X5 X8', 'X5 X9', 'X5 X10', 'X5 X11', 'X6^2', 'X6 X7', 'X6 X8', 'X6 X9',\n",
       "       'X6 X10', 'X6 X11', 'X6 X18', 'X6 X20', 'X6 X21', 'X7^2', 'X7 X8',\n",
       "       'X7 X9', 'X7 X10', 'X7 X11', 'X7 X18', 'X7 X20', 'X8^2', 'X8 X9',\n",
       "       'X8 X10', 'X8 X11', 'X8 X19', 'X9^2', 'X9 X10', 'X9 X11', 'X9 X20',\n",
       "       'X10^2', 'X10 X11', 'X11^2', 'X11 X22'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#setting parameters to choose the best 50 variables based on the significance of the variable F stat\n",
    "selector = SelectKBest(f_classif, k=50) #setting parameters to choose the best 50 variables based on the significance of the variable F stat\n",
    "\n",
    "#Fitting selector object to training set\n",
    "selector.fit(X_train, y_train)\n",
    "\n",
    "selected_columns = X_train.columns[selector.get_support()]\n",
    "removed_columns = X_train.columns[~selector.get_support()]\n",
    "selected_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_kb50 = X_train[selected_columns]\n",
    "X_test_kb50 = X_test[selected_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select K Best = 75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SelectKBest(k=75, score_func=<function f_classif at 0x7fa472f3d510>)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#setting parameters to choose the best 75 variables based on the significance of the variable F stat\n",
    "selector = SelectKBest(f_classif, k=75) #setting parameters to choose the best 50 variables based on the significance of the variable F stat\n",
    "\n",
    "#Fitting selector object to training set\n",
    "selector.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['X1', 'X6', 'X7', 'X8', 'X9', 'X10', 'X11', 'X18', 'X19', 'X20', 'X23',\n",
       "       'X1^2', 'X1 X5', 'X1 X6', 'X1 X7', 'X1 X8', 'X1 X9', 'X1 X10', 'X1 X11',\n",
       "       'X1 X18', 'X5 X6', 'X5 X7', 'X5 X8', 'X5 X9', 'X5 X10', 'X5 X11',\n",
       "       'X5 X18', 'X5 X19', 'X6^2', 'X6 X7', 'X6 X8', 'X6 X9', 'X6 X10',\n",
       "       'X6 X11', 'X6 X18', 'X6 X19', 'X6 X20', 'X6 X21', 'X6 X22', 'X7^2',\n",
       "       'X7 X8', 'X7 X9', 'X7 X10', 'X7 X11', 'X7 X18', 'X7 X19', 'X7 X20',\n",
       "       'X7 X21', 'X7 X22', 'X8^2', 'X8 X9', 'X8 X10', 'X8 X11', 'X8 X18',\n",
       "       'X8 X19', 'X8 X20', 'X8 X21', 'X8 X22', 'X9^2', 'X9 X10', 'X9 X11',\n",
       "       'X9 X18', 'X9 X20', 'X9 X21', 'X9 X22', 'X10^2', 'X10 X11', 'X10 X18',\n",
       "       'X10 X20', 'X10 X21', 'X10 X22', 'X11^2', 'X11 X20', 'X11 X21',\n",
       "       'X11 X22'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_columns = X_train.columns[selector.get_support()]\n",
    "removed_columns = X_train.columns[~selector.get_support()]\n",
    "selected_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_kb75 = X_train[selected_columns]\n",
    "X_test_kb75 = X_test[selected_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select K Best = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting parameters to choose the best 100 variables based on the significance of the variable F stat\n",
    "selector = SelectKBest(f_classif, k=100) #setting parameters to choose the best 50 variables based on the significance of the variable F stat\n",
    "\n",
    "#Fitting selector object to training set\n",
    "selector.fit(X_train, y_train)\n",
    "\n",
    "selected_columns = X_train.columns[selector.get_support()]\n",
    "removed_columns = X_train.columns[~selector.get_support()]\n",
    "selected_columns\n",
    "\n",
    "X_train_kb100 = X_train[selected_columns]\n",
    "X_test_kb100 = X_test[selected_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Fitting and Hyperparameter Tuning\n",
    "KNN, Logistic Regression, Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN --- scaling with minmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1score - all scaled:0.3734068205304857\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import metrics\n",
    "\n",
    "scaler = MinMaxScaler()  \n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train_scale1 = scaler.transform(X_train)  \n",
    "X_test_scale1 = scaler.transform(X_test)\n",
    "\n",
    "knn_all = KNeighborsClassifier(n_neighbors=1, n_jobs = -1)\n",
    "\n",
    "knn_all.fit(X_train_scale1, y_train)\n",
    "\n",
    "y_pred_scale1 =knn_all.predict(X_test_scale1)\n",
    "\n",
    "print('f1score - all scaled:' + str(metrics.f1_score(y_test, y_pred_scale1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1score - Kb50 scaled:0.38469422617014004\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler()  \n",
    "scaler.fit(X_train_kb50)\n",
    "\n",
    "X_train_scale2 = scaler.transform(X_train_kb50)  \n",
    "X_test_scale2 = scaler.transform(X_test_kb50)\n",
    "\n",
    "knn_kb50 = KNeighborsClassifier(n_neighbors=1)\n",
    "\n",
    "knn_kb50.fit(X_train_scale2, y_train)\n",
    "\n",
    "y_pred_scale2 =knn_kb50.predict(X_test_scale2)\n",
    "\n",
    "print('f1score - Kb50 scaled:' + str(metrics.f1_score(y_test, y_pred_scale2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1score - kb75 scaled:0.40654664484451714\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler()  \n",
    "scaler.fit(X_train_kb75)\n",
    "\n",
    "X_train_scale3 = scaler.transform(X_train_kb75)  \n",
    "X_test_scale3 = scaler.transform(X_test_kb75)\n",
    "\n",
    "knn_kb75 = KNeighborsClassifier(n_neighbors=1)\n",
    "\n",
    "knn_kb75.fit(X_train_scale3, y_train)\n",
    "\n",
    "y_pred_scale3 =knn_kb75.predict(X_test_scale3)\n",
    "\n",
    "print('f1score - kb75 scaled:' + str(metrics.f1_score(y_test, y_pred_scale3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1score - kb100 scaled:0.40480961923847697\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler()  \n",
    "scaler.fit(X_train_kb100)\n",
    "\n",
    "X_train_scale4 = scaler.transform(X_train_kb100)  \n",
    "X_test_scale4 = scaler.transform(X_test_kb100)\n",
    "\n",
    "knn_kb100 = KNeighborsClassifier(n_neighbors=1)\n",
    "\n",
    "knn_kb100.fit(X_train_scale4, y_train)\n",
    "\n",
    "y_pred_scale4 =knn_kb100.predict(X_test_scale4)\n",
    "\n",
    "print('f1score - kb100 scaled:' + str(metrics.f1_score(y_test, y_pred_scale4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optimal K for KNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fumction to find the position of the maximum value in a list\n",
    "def max_value(l):\n",
    "    max_val = max(l)\n",
    "    max_idx = l.index(max_val)\n",
    "    return max_idx, max_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.7330269789504892\n"
     ]
    }
   ],
   "source": [
    "#create a container to track the scores\n",
    "k_scores2=[]\n",
    "\n",
    "#set up a loop to fit the model using a different values of K\n",
    "\n",
    "k_range = list(range(1, 21))\n",
    "for k in k_range:\n",
    "    knn_kb50_k = KNeighborsClassifier(n_neighbors=k)\n",
    "    #fit the model and get the score on a evaluation metric\n",
    "    knn_kb50_k.fit(X_train_scale2, y_train)\n",
    "    y_pred_knn2 = knn_kb50_k.predict(X_test_scale2)\n",
    "    \n",
    "    acc2 = metrics.accuracy_score(y_test, y_pred_scale2)\n",
    "    \n",
    "    k_scores2.append(acc2)\n",
    "\n",
    "#use the max_value function to find the K value that gives you the best accuracy pred \n",
    "idx, val = max_value(k_scores2)\n",
    "    \n",
    "print(idx+1, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1score with k=1:  0.4442462087421945\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "\n",
    "knn_kb50_k.fit(X_train_scale2, y_train)\n",
    "\n",
    "y_pred_knn2 =knn_kb50_k.predict(X_test_scale2)\n",
    "\n",
    "print('f1score with k=1:  ' + str(metrics.f1_score(y_test, y_pred_knn2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 0.8158908983101097\n"
     ]
    }
   ],
   "source": [
    "# need to scale the X_train all data\n",
    "\n",
    "\n",
    "#create a container to track the scores\n",
    "k_scores3=[]\n",
    "\n",
    "#set up a loop to fit the model using a different values of K\n",
    "\n",
    "k_range = list(range(1, 21))\n",
    "for k in k_range:\n",
    "    knn_kb75_k = KNeighborsClassifier(n_neighbors=k)\n",
    "    #fit the model and get the score on a evaluation metric\n",
    "    knn_kb75_k.fit(X_train_scale3, y_train)\n",
    "    y_pred_knn3 = knn_kb75_k.predict(X_test_scale3)\n",
    "    \n",
    "    acc3 = metrics.accuracy_score(y_test, y_pred_knn3)\n",
    "    \n",
    "    k_scores3.append(acc3)\n",
    "\n",
    "#use the max_value function to find the K value that gives you the best accuracy pred \n",
    "idx, val = max_value(k_scores3)\n",
    "    \n",
    "print(idx+1, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1score with k=20:  0.4472049689440994\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=17)\n",
    "\n",
    "knn_kb75_k.fit(X_train_knn3, y_train)\n",
    "\n",
    "y_pred_knn3 =knn_kb75_k.predict(X_test_knn3)\n",
    "\n",
    "print('f1score with k=17:  ' + str(metrics.f1_score(y_test, y_pred_knn3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 0.8055143788911948\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler()  \n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train_knn4 = scaler.transform(X_train)  \n",
    "X_test_knn4 = scaler.transform(X_test)\n",
    "\n",
    "#create a container to track the scores\n",
    "k_scores4=[]\n",
    "\n",
    "#set up a loop to fit the model using a different values of K\n",
    "\n",
    "k_range = list(range(1, 21))\n",
    "for k in k_range:\n",
    "    knn_kb100_k = KNeighborsClassifier(n_neighbors=k)\n",
    "    #fit the model and get the score on a evaluation metric\n",
    "    knn_kb100_k.fit(X_train_scale4, y_train)\n",
    "    y_pred_knn4 = knn_kb100_k.predict(X_test_scale4)\n",
    "    \n",
    "    acc4 = metrics.accuracy_score(y_test, y_pred_knn4)\n",
    "    \n",
    "    k_scores4.append(acc4)\n",
    "\n",
    "#use the max_value function to find the K value that gives you the best accuracy pred \n",
    "idx, val = max_value(k_scores4)\n",
    "    \n",
    "print(idx+1, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1score with k=20:  0.3396603396603397\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=17)\n",
    "\n",
    "knn_kb100_k.fit(X_train_scale4, y_train)\n",
    "\n",
    "y_pred_knn4 =knn_kb100_k.predict(X_test_scale4)\n",
    "\n",
    "print('f1score with k=17:  ' + str(metrics.f1_score(y_test, y_pred_knn4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8147050103765194\n",
      "0.46259673258813416\n",
      "0.6521570271613826\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg_all = LogisticRegression(random_state = 25, max_iter=1000, C =1e5)\n",
    "\n",
    "logreg_all.fit(X_train_scale1, y_train) #fit logreg model on n_features = 50\n",
    "\n",
    "# class predictions (not predicted probabilities)\n",
    "y_pred_all = logreg_all.predict(X_test_scale1)\n",
    "\n",
    "print(metrics.accuracy_score(y_test, y_pred_all))\n",
    "print(metrics.f1_score(y_test, y_pred_all))\n",
    "print(metrics.roc_auc_score(y_test, y_pred_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8013637711236288\n",
      "0.3266331658291457\n",
      "0.5924315955996046\n"
     ]
    }
   ],
   "source": [
    "logreg_kb75 = LogisticRegression(random_state = 25)\n",
    "\n",
    "logreg_kb75.fit(X_train_kb75, y_train) #fit logreg model on n_features = 50\n",
    "\n",
    "# class predictions (not predicted probabilities)\n",
    "y_pred_kb75 = logreg_kb75.predict(X_test_kb75)\n",
    "\n",
    "print(metrics.accuracy_score(y_test, y_pred_kb75))\n",
    "print(metrics.f1_score(y_test, y_pred_kb75))\n",
    "print(metrics.roc_auc_score(y_test, y_pred_kb75))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8193003261191817\n",
      "0.46511627906976744\n",
      "0.6531850155803198\n"
     ]
    }
   ],
   "source": [
    "## Scaled kb75\n",
    "logreg_scaled = LogisticRegression(random_state = 25, max_iter= 10000)\n",
    "\n",
    "logreg_scaled.fit(X_train_scale3, y_train) #fit logreg model on n_features = 50\n",
    "\n",
    "# class predictions (not predicted probabilities)\n",
    "y_pred_k50 = logreg_scaled.predict(X_test_scale3)\n",
    "\n",
    "print(metrics.accuracy_score(y_test, y_pred_k50))\n",
    "print(metrics.f1_score(y_test, y_pred_k50))\n",
    "print(metrics.roc_auc_score(y_test, y_pred_k50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weighted Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7539282537800178\n",
      "0.5174418604651162\n",
      "0.6976962313047329\n"
     ]
    }
   ],
   "source": [
    "w = {0:20, 1:80}\n",
    "\n",
    "logreg_w1 = LogisticRegression(random_state=25, class_weight=w, max_iter=10000, C=1e5)\n",
    "\n",
    "logreg_w1.fit(X_train_scale3, y_train) #fit logreg model on n_features = 50\n",
    "\n",
    "y_pred_w = logreg_w1.predict(X_test_scale3)\n",
    "\n",
    "print(metrics.accuracy_score(y_test, y_pred_w))\n",
    "print(metrics.f1_score(y_test, y_pred_w))\n",
    "print(metrics.roc_auc_score(y_test, y_pred_w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.768900088941595\n",
      "0.515387006527821\n",
      "0.692654053084763\n"
     ]
    }
   ],
   "source": [
    "w = {0:25, 1:75}\n",
    "\n",
    "logreg_w2 = LogisticRegression(random_state=25, class_weight=w, max_iter=10000, C=1e5)\n",
    "\n",
    "logreg_w2.fit(X_train_scale3, y_train) #fit logreg model on n_features = 50\n",
    "\n",
    "y_pred_w = logreg_w2.predict(X_test_scale3)\n",
    "\n",
    "print(metrics.accuracy_score(y_test, y_pred_w))\n",
    "print(metrics.f1_score(y_test, y_pred_w))\n",
    "print(metrics.roc_auc_score(y_test, y_pred_w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7469611621701749\n",
      "0.5198312236286919\n",
      "0.7013910293349057\n"
     ]
    }
   ],
   "source": [
    "\n",
    "logreg_all_w = LogisticRegression(random_state = 25, max_iter=1000, C =1e5, class_weight={0:20,1:80})\n",
    "\n",
    "logreg_all_w.fit(X_train_scale1, y_train) #fit logreg model on n_features = 50\n",
    "\n",
    "# class predictions (not predicted probabilities)\n",
    "y_pred_all = logreg_all_w.predict(X_test_scale1)\n",
    "\n",
    "print(metrics.accuracy_score(y_test, y_pred_all))\n",
    "print(metrics.f1_score(y_test, y_pred_all))\n",
    "print(metrics.roc_auc_score(y_test, y_pred_all))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                       random_state=None, splitter='best')"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training F1 Score: 0.9987253930038239\n",
      "Testing F1 Score: 0.38939197930142305\n",
      "Testing roc_auc score: 0.606825603159075\n"
     ]
    }
   ],
   "source": [
    "clf1_all = DecisionTreeClassifier()\n",
    "\n",
    "#Train Decision Tree Classifier\n",
    "clf1_all.fit(X_train,y_train)\n",
    "\n",
    "#predict the training set\n",
    "y_pred_train = clf1_all.predict(X_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred_test = clf1_all.predict(X_test)\n",
    "\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Training F1 Score:\",metrics.f1_score(y_train, y_pred_train))\n",
    "print(\"Testing F1 Score:\",metrics.f1_score(y_test, y_pred_test))\n",
    "print(\"Testing roc_auc score:\", metrics.roc_auc_score(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training F1 Score: 0.9987253930038239\n",
      "Testing F1 Score: 0.38351719662556777\n",
      "Testing roc_auc score: 0.6030416562170358\n"
     ]
    }
   ],
   "source": [
    "clf2_scale1 = DecisionTreeClassifier()\n",
    "\n",
    "#Train Decision Tree Classifier\n",
    "clf2_scale1.fit(X_train_scale1,y_train)\n",
    "\n",
    "#predict the training set\n",
    "y_pred_train = clf2_scale1.predict(X_train_scale1)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred_test = clf2_scale1.predict(X_test_scale1)\n",
    "\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Training F1 Score:\",metrics.f1_score(y_train, y_pred_train))\n",
    "print(\"Testing F1 Score:\",metrics.f1_score(y_test, y_pred_test))\n",
    "print(\"Testing roc_auc score:\", metrics.roc_auc_score(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training F1 Score: 0.9923142613151152\n",
      "Testing F1 Score: 0.38390501319261217\n",
      "Testing roc_auc score: 0.603924594187389\n"
     ]
    }
   ],
   "source": [
    "clf3_scale3 = DecisionTreeClassifier()\n",
    "\n",
    "#Train Decision Tree Classifier\n",
    "clf3_scale3.fit(X_train_scale3,y_train)\n",
    "\n",
    "#predict the training set\n",
    "y_pred_train = clf3_scale3.predict(X_train_scale3)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred_test = clf3_scale3.predict(X_test_scale3)\n",
    "\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Training F1 Score:\",metrics.f1_score(y_train, y_pred_train))\n",
    "print(\"Testing F1 Score:\",metrics.f1_score(y_test, y_pred_test))\n",
    "print(\"Testing roc_auc score:\", metrics.roc_auc_score(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training F1 Score: 0.52838519764508\n",
      "Testing F1 Score: 0.4986065366100836\n",
      "Testing roc_auc score: 0.6899227907884988\n"
     ]
    }
   ],
   "source": [
    "w = {0:20, 1:80}\n",
    "clf4_scale3 = DecisionTreeClassifier(class_weight=w, max_depth= 6, random_state = 100)\n",
    "\n",
    "#Train Decision Tree Classifier\n",
    "clf4_scale3.fit(X_train_scale3,y_train)\n",
    "\n",
    "#predict the training set\n",
    "y_pred_train = clf4_scale3.predict(X_train_scale3)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred_test = clf4_scale3.predict(X_test_scale3)\n",
    "\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Training F1 Score:\",metrics.f1_score(y_train, y_pred_train))\n",
    "print(\"Testing F1 Score:\",metrics.f1_score(y_test, y_pred_test))\n",
    "print(\"Testing roc_auc score:\", metrics.roc_auc_score(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training F1 Score: 0.5550307038261691\n",
      "Testing F1 Score: 0.5048599199542595\n",
      "Testing roc_auc score: 0.6891642273558893\n"
     ]
    }
   ],
   "source": [
    "w = {0:20, 1:80}\n",
    "clf5_scale1 = DecisionTreeClassifier(class_weight=w, max_depth= 6, random_state = 100)\n",
    "\n",
    "#Train Decision Tree Classifier\n",
    "clf5_scale1.fit(X_train_scale1,y_train)\n",
    "\n",
    "#predict the training set\n",
    "y_pred_train = clf5_scale1.predict(X_train_scale1)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred_test = clf5_scale1.predict(X_test_scale1)\n",
    "\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Training F1 Score:\",metrics.f1_score(y_train, y_pred_train))\n",
    "print(\"Testing F1 Score:\",metrics.f1_score(y_test, y_pred_test))\n",
    "print(\"Testing roc_auc score:\", metrics.roc_auc_score(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search CV --- DecisionTreeClassifier with scale 1 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Grid search\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 180 candidates, totalling 1800 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    4.9s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   16.7s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1242 tasks      | elapsed:  4.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1792 tasks      | elapsed:  7.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1800 out of 1800 | elapsed:  7.9min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score=nan,\n",
       "             estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None,\n",
       "                                              criterion='gini', max_depth=None,\n",
       "                                              max_features=None,\n",
       "                                              max_leaf_nodes=None,\n",
       "                                              min_impurity_decrease=0.0,\n",
       "                                              min_impurity_split=None,\n",
       "                                              min_samples_leaf=1,\n",
       "                                              min_samples_split=2,\n",
       "                                              min_weight_fraction_leaf=0.0,\n",
       "                                              presort='deprecated',\n",
       "                                              random_state=1, splitter='best'),\n",
       "             iid='deprecated', n_jobs=-1,\n",
       "             param_grid={'class_weight': [{0: 20, 1: 80}],\n",
       "                         'criterion': ['gini', 'entropy'],\n",
       "                         'max_depth': range(1, 11),\n",
       "                         'min_samples_leaf': range(2, 11)},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='f1', verbose=1)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# w = {0:20, 1:80}\n",
    "parameters1={'class_weight': [{0:20, 1:80}],\n",
    "             'max_depth': range(1,11,1), \n",
    "             'criterion': ['gini', 'entropy'], \n",
    "             'min_samples_leaf': range(2,11,1), \n",
    "             'random_state': 100 }\n",
    "\n",
    "#create our estimaor\n",
    "clf6_all = DecisionTreeClassifier()\n",
    "\n",
    "#create the instance of GridSearchCV\n",
    "grid_tree = GridSearchCV(clf2_all, parameters1, cv=10, scoring='f1', verbose=1, n_jobs=-1)\n",
    "\n",
    "#fit the Gridsearch to our data\n",
    "grid_tree.fit(X_train_scale1,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5165993830749158\n",
      "{'class_weight': {0: 20, 1: 80}, 'criterion': 'gini', 'max_depth': 3, 'min_samples_leaf': 9}\n",
      "DecisionTreeClassifier(ccp_alpha=0.0, class_weight={0: 20, 1: 80},\n",
      "                       criterion='gini', max_depth=3, max_features=None,\n",
      "                       max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "                       min_impurity_split=None, min_samples_leaf=9,\n",
      "                       min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                       presort='deprecated', random_state=1, splitter='best')\n"
     ]
    }
   ],
   "source": [
    "# examine the best model\n",
    "\n",
    "# Single best score achieved across all params (min_samples_split)\n",
    "print(grid_tree.best_score_)\n",
    "\n",
    "# Dictionary containing the parameters (min_samples_split) used to generate that score\n",
    "print(grid_tree.best_params_)\n",
    "\n",
    "# Actual model object fit with those best parameters\n",
    "# Shows default parameters that we did not specify\n",
    "print(grid_tree.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.5029868578255675\n"
     ]
    }
   ],
   "source": [
    "#Predict the response for test dataset\n",
    "y_pred = grid_tree.best_estimator_.predict(X_test_scale1)\n",
    "\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"F1:\",metrics.f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search CV --- Decison Tree with scale 3 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 50 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  76 tasks      | elapsed:    1.8s\n",
      "[Parallel(n_jobs=-1)]: Done 376 tasks      | elapsed:   11.1s\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:   16.7s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score=nan,\n",
       "             estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None,\n",
       "                                              criterion='gini', max_depth=None,\n",
       "                                              max_features=None,\n",
       "                                              max_leaf_nodes=None,\n",
       "                                              min_impurity_decrease=0.0,\n",
       "                                              min_impurity_split=None,\n",
       "                                              min_samples_leaf=1,\n",
       "                                              min_samples_split=2,\n",
       "                                              min_weight_fraction_leaf=0.0,\n",
       "                                              presort='deprecated',\n",
       "                                              random_state=None,\n",
       "                                              splitter='best'),\n",
       "             iid='deprecated', n_jobs=-1,\n",
       "             param_grid={'class_weight': [{0: 20, 1: 80}],\n",
       "                         'criterion': ['gini', 'entropy'],\n",
       "                         'max_depth': range(1, 6),\n",
       "                         'min_samples_leaf': range(6, 11),\n",
       "                         'random_state': [100]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='f1', verbose=1)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters2={'class_weight': [{0:20, 1:80}],\n",
    "             'max_depth': range(1,6,1), \n",
    "             'criterion': ['gini', 'entropy'], \n",
    "             'min_samples_leaf': range(6,11,1), \n",
    "             'random_state' :[100]}\n",
    "\n",
    "#create our estimaor\n",
    "clf7_scale3 = DecisionTreeClassifier()\n",
    "\n",
    "#create the instance of GridSearchCV\n",
    "grid_tree = GridSearchCV(clf7_scale3, parameters2, cv=10, scoring='f1', verbose=1, n_jobs=-1)\n",
    "\n",
    "#fit the Gridsearch to our data\n",
    "grid_tree.fit(X_train_scale3,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5235950461012939\n",
      "{'class_weight': {0: 20, 1: 80}, 'criterion': 'gini', 'max_depth': 3, 'min_samples_leaf': 6, 'random_state': 100}\n",
      "DecisionTreeClassifier(ccp_alpha=0.0, class_weight={0: 20, 1: 80},\n",
      "                       criterion='gini', max_depth=3, max_features=None,\n",
      "                       max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "                       min_impurity_split=None, min_samples_leaf=6,\n",
      "                       min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                       presort='deprecated', random_state=100, splitter='best')\n"
     ]
    }
   ],
   "source": [
    "# examine the best model\n",
    "\n",
    "# Single best score achieved across all params (min_samples_split)\n",
    "print(grid_tree.best_score_)\n",
    "\n",
    "# Dictionary containing the parameters (min_samples_split) used to generate that score\n",
    "print(grid_tree.best_params_)\n",
    "\n",
    "# Actual model object fit with those best parameters\n",
    "# Shows default parameters that we did not specify\n",
    "print(grid_tree.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.503868771278242\n"
     ]
    }
   ],
   "source": [
    "#Predict the response for test dataset\n",
    "y_pred = grid_tree.best_estimator_.predict(X_test_scale3)\n",
    "\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"F1:\",metrics.f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search CV --- Decison Tree with scale 1- max_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 150 candidates, totalling 1500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  76 tasks      | elapsed:    2.1s\n",
      "[Parallel(n_jobs=-1)]: Done 376 tasks      | elapsed:   12.8s\n",
      "[Parallel(n_jobs=-1)]: Done 876 tasks      | elapsed:   42.8s\n",
      "[Parallel(n_jobs=-1)]: Done 1440 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1500 out of 1500 | elapsed:  1.6min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score=nan,\n",
       "             estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None,\n",
       "                                              criterion='gini', max_depth=None,\n",
       "                                              max_features=None,\n",
       "                                              max_leaf_nodes=None,\n",
       "                                              min_impurity_decrease=0.0,\n",
       "                                              min_impurity_split=None,\n",
       "                                              min_samples_leaf=1,\n",
       "                                              min_samples_split=2,\n",
       "                                              min_weight_fraction_leaf=0.0,\n",
       "                                              presort='deprecated',\n",
       "                                              random_state=None,\n",
       "                                              splitter='best'),\n",
       "             iid='deprecated', n_jobs=-1,\n",
       "             param_grid={'class_weight': [{0: 20, 1: 80}],\n",
       "                         'criterion': ['gini', 'entropy'],\n",
       "                         'max_depth': range(1, 6), 'max_features': [65, 70, 75],\n",
       "                         'min_samples_leaf': range(6, 11),\n",
       "                         'random_state': [100]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='f1', verbose=1)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters3={'class_weight': [{0:20, 1:80}],\n",
    "             'max_depth': range(1,6,1), \n",
    "             'criterion': ['gini', 'entropy'], \n",
    "             'min_samples_leaf': range(6,11,1), \n",
    "             'random_state' :[100],\n",
    "            'max_features': [65,70,75]}\n",
    "\n",
    "#create our estimaor\n",
    "clf8_scale1 = DecisionTreeClassifier()\n",
    "\n",
    "#create the instance of GridSearchCV\n",
    "grid_tree = GridSearchCV(clf8_scale1, parameters3, cv=10, scoring='f1', verbose=1, n_jobs=-1)\n",
    "\n",
    "#fit the Gridsearch to our data\n",
    "grid_tree.fit(X_train_scale1,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5240605783796568\n",
      "{'class_weight': {0: 20, 1: 80}, 'criterion': 'gini', 'max_depth': 3, 'max_features': 75, 'min_samples_leaf': 6, 'random_state': 100}\n",
      "DecisionTreeClassifier(ccp_alpha=0.0, class_weight={0: 20, 1: 80},\n",
      "                       criterion='gini', max_depth=3, max_features=75,\n",
      "                       max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "                       min_impurity_split=None, min_samples_leaf=6,\n",
      "                       min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                       presort='deprecated', random_state=100, splitter='best')\n"
     ]
    }
   ],
   "source": [
    "# examine the best model\n",
    "\n",
    "# Single best score achieved across all params (min_samples_split)\n",
    "print(grid_tree.best_score_)\n",
    "\n",
    "# Dictionary containing the parameters (min_samples_split) used to generate that score\n",
    "print(grid_tree.best_params_)\n",
    "\n",
    "# Actual model object fit with those best parameters\n",
    "# Shows default parameters that we did not specify\n",
    "print(grid_tree.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.503868771278242\n"
     ]
    }
   ],
   "source": [
    "#Predict the response for test dataset\n",
    "y_pred = grid_tree.best_estimator_.predict(X_test_scale1)\n",
    "\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"F1:\",metrics.f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search CV --- Logistic Regression with scale 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 14 candidates, totalling 140 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 104 tasks      | elapsed:   25.9s\n",
      "[Parallel(n_jobs=-1)]: Done 140 out of 140 | elapsed:   59.8s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score=nan,\n",
       "             estimator=LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                          fit_intercept=True,\n",
       "                                          intercept_scaling=1, l1_ratio=None,\n",
       "                                          max_iter=100, multi_class='auto',\n",
       "                                          n_jobs=None, penalty='l2',\n",
       "                                          random_state=None, solver='lbfgs',\n",
       "                                          tol=0.0001, verbose=0,\n",
       "                                          warm_start=False),\n",
       "             iid='deprecated', n_jobs=-1,\n",
       "             param_grid={'C': array([1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02, 1.e+03]),\n",
       "                         'class_weight': [{0: 20, 1: 80}], 'max_iter': [10000],\n",
       "                         'penalty': ['l1', 'l2'], 'random_state': [100]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='f1', verbose=1)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters3={'class_weight': [{0:20, 1:80}],\n",
    "             'max_iter': [10000], \n",
    "             'penalty': ['l1', 'l2'],  \n",
    "             'random_state' :[100],\n",
    "            \"C\":[1e5]}\n",
    "\n",
    "#create our estimaor\n",
    "logreg_w3 = LogisticRegression()\n",
    "\n",
    "#create the instance of GridSearchCV\n",
    "grid_tree = GridSearchCV(logreg_w3, parameters3, cv=10, scoring='f1', verbose=1, n_jobs=-1)\n",
    "\n",
    "#fit the Gridsearch to our data\n",
    "grid_tree.fit(X_train_scale1,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5280412262296301\n",
      "{'C': 100.0, 'class_weight': {0: 20, 1: 80}, 'max_iter': 10000, 'penalty': 'l2', 'random_state': 100}\n",
      "LogisticRegression(C=100.0, class_weight={0: 20, 1: 80}, dual=False,\n",
      "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
      "                   max_iter=10000, multi_class='auto', n_jobs=None,\n",
      "                   penalty='l2', random_state=100, solver='lbfgs', tol=0.0001,\n",
      "                   verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "# examine the best model\n",
    "\n",
    "# Single best score achieved across all params (min_samples_split)\n",
    "print(grid_tree.best_score_)\n",
    "\n",
    "# Dictionary containing the parameters (min_samples_split) used to generate that score\n",
    "print(grid_tree.best_params_)\n",
    "\n",
    "# Actual model object fit with those best parameters\n",
    "# Shows default parameters that we did not specify\n",
    "print(grid_tree.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.5196850393700788\n"
     ]
    }
   ],
   "source": [
    "#Predict the response for test dataset\n",
    "y_pred = grid_tree.best_estimator_.predict(X_test_scale1)\n",
    "\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"F1:\",metrics.f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search CV --- Logistic Regression with scale 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 2 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    5.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score=nan,\n",
       "             estimator=LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                          fit_intercept=True,\n",
       "                                          intercept_scaling=1, l1_ratio=None,\n",
       "                                          max_iter=100, multi_class='auto',\n",
       "                                          n_jobs=None, penalty='l2',\n",
       "                                          random_state=None, solver='lbfgs',\n",
       "                                          tol=0.0001, verbose=0,\n",
       "                                          warm_start=False),\n",
       "             iid='deprecated', n_jobs=-1,\n",
       "             param_grid={'C': [100000.0], 'class_weight': [{0: 20, 1: 80}],\n",
       "                         'max_iter': [10000], 'penalty': ['l1', 'l2'],\n",
       "                         'random_state': [100]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='f1', verbose=1)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters3={'class_weight': [{0:20, 1:80}],\n",
    "             'max_iter': [10000], \n",
    "             'penalty': ['l1', 'l2'],  \n",
    "             'random_state' :[100],\n",
    "            \"C\":[1e5]}\n",
    "\n",
    "#create our estimaor\n",
    "logreg_w4 = LogisticRegression()\n",
    "\n",
    "#create the instance of GridSearchCV\n",
    "grid_tree = GridSearchCV(logreg_w4, parameters3, cv=10, scoring='f1', verbose=1, n_jobs=-1)\n",
    "\n",
    "#fit the Gridsearch to our data\n",
    "grid_tree.fit(X_train_scale3,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5267661561629272\n",
      "{'C': 100000.0, 'class_weight': {0: 20, 1: 80}, 'max_iter': 10000, 'penalty': 'l2', 'random_state': 100}\n",
      "LogisticRegression(C=100000.0, class_weight={0: 20, 1: 80}, dual=False,\n",
      "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
      "                   max_iter=10000, multi_class='auto', n_jobs=None,\n",
      "                   penalty='l2', random_state=100, solver='lbfgs', tol=0.0001,\n",
      "                   verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "# examine the best model\n",
    "\n",
    "# Single best score achieved across all params (min_samples_split)\n",
    "print(grid_tree.best_score_)\n",
    "\n",
    "# Dictionary containing the parameters (min_samples_split) used to generate that score\n",
    "print(grid_tree.best_params_)\n",
    "\n",
    "# Actual model object fit with those best parameters\n",
    "# Shows default parameters that we did not specify\n",
    "print(grid_tree.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.5174418604651162\n"
     ]
    }
   ],
   "source": [
    "#Predict the response for test dataset\n",
    "y_pred = grid_tree.best_estimator_.predict(X_test_scale3)\n",
    "\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"F1:\",metrics.f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the classifier using 100 trees\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "rfc = RandomForestClassifier(class_weight= {0: 20, 1: 80}, \n",
    "                             criterion='gini', \n",
    "                             max_depth= 3, \n",
    "                             oob_score=True,\n",
    "                             min_samples_leaf= 6, \n",
    "                             random_state=100, \n",
    "                              n_estimators=150,\n",
    "                             verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
       "                       class_weight={0: 20, 1: 80}, criterion='gini',\n",
       "                       max_depth=3, max_features='auto', max_leaf_nodes=None,\n",
       "                       max_samples=None, min_impurity_decrease=0.0,\n",
       "                       min_impurity_split=None, min_samples_leaf=6,\n",
       "                       min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                       n_estimators=150, n_jobs=None, oob_score=True,\n",
       "                       random_state=100, verbose=1, warm_start=False)"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 150 out of 150 | elapsed:    1.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test F1 score:  0.518290808484476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 150 out of 150 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "rfc.fit(X_train_scale1, y_train)\n",
    "\n",
    "#use the fitted model to predict on the test data\n",
    "rfc_preds = rfc.predict(X_test_scale1)\n",
    "\n",
    "rfc_f1 = metrics.f1_score(y_test, rfc_preds)\n",
    "\n",
    "# checking accuracy on the test data\n",
    "print('Test F1 score: ', rfc_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc1 = RandomForestClassifier(class_weight= {0: 20, 1: 80}, \n",
    "                             criterion='gini', \n",
    "                             max_depth= 3, \n",
    "                             oob_score=True,\n",
    "                             min_samples_leaf= 6, \n",
    "                             random_state=100, \n",
    "                              n_estimators=150,\n",
    "                             verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 150 out of 150 | elapsed:    0.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test F1 score:  0.515451174289246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 150 out of 150 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "rfc1.fit(X_train_scale3, y_train)\n",
    "\n",
    "#use the fitted model to predict on the test data\n",
    "rfc_preds = rfc1.predict(X_test_scale3)\n",
    "\n",
    "rfc_f1 = metrics.f1_score(y_test, rfc_preds)\n",
    "\n",
    "# checking accuracy on the test data\n",
    "print('Test F1 score: ', rfc_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search CV --- Random Forest with scale 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters4={'class_weight': [{0:20, 1:80}],\n",
    "             'max_depth': range(1,6,1), \n",
    "             'criterion': ['gini', 'entropy'], \n",
    "             'min_samples_leaf': range(6,11,1), \n",
    "             'random_state' :[100],                             \n",
    "             'n_estimators':[150]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_tree=GridSearchCV(RandomForestClassifier(), parameters4, cv=5, scoring='f1', verbose=1, n_jobs=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   25.2s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done 250 out of 250 | elapsed:  3.8min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score=nan,\n",
       "             estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
       "                                              class_weight=None,\n",
       "                                              criterion='gini', max_depth=None,\n",
       "                                              max_features='auto',\n",
       "                                              max_leaf_nodes=None,\n",
       "                                              max_samples=None,\n",
       "                                              min_impurity_decrease=0.0,\n",
       "                                              min_impurity_split=None,\n",
       "                                              min_samples_leaf=1,\n",
       "                                              min_samples_split=2,\n",
       "                                              min_weight_fraction_leaf=0.0,\n",
       "                                              n_estimators=100, n_jobs=None,\n",
       "                                              oob_score=False,\n",
       "                                              random_state=None, verbose=0,\n",
       "                                              warm_start=False),\n",
       "             iid='deprecated', n_jobs=-1,\n",
       "             param_grid={'class_weight': [{0: 20, 1: 80}],\n",
       "                         'criterion': ['gini', 'entropy'],\n",
       "                         'max_depth': range(1, 6),\n",
       "                         'min_samples_leaf': range(6, 11),\n",
       "                         'n_estimators': [150], 'random_state': [100]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='f1', verbose=1)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "grid_tree.fit(X_train_scale1, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5313646725102872\n",
      "{'class_weight': {0: 20, 1: 80}, 'criterion': 'entropy', 'max_depth': 3, 'min_samples_leaf': 6, 'n_estimators': 150, 'random_state': 100}\n",
      "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
      "                       class_weight={0: 20, 1: 80}, criterion='entropy',\n",
      "                       max_depth=3, max_features='auto', max_leaf_nodes=None,\n",
      "                       max_samples=None, min_impurity_decrease=0.0,\n",
      "                       min_impurity_split=None, min_samples_leaf=6,\n",
      "                       min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                       n_estimators=150, n_jobs=None, oob_score=False,\n",
      "                       random_state=100, verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "# examine the best model\n",
    "\n",
    "# Single best score achieved across all params (min_samples_split)\n",
    "print(grid_tree.best_score_)\n",
    "\n",
    "# Dictionary containing the parameters (min_samples_split) used to generate that score\n",
    "print(grid_tree.best_params_)\n",
    "\n",
    "# Actual model object fit with those best parameters\n",
    "# Shows default parameters that we did not specify\n",
    "print(grid_tree.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.514710436667699\n"
     ]
    }
   ],
   "source": [
    "#Predict the response for test dataset\n",
    "y_pred = grid_tree.best_estimator_.predict(X_test_scale1)\n",
    "\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"F1:\",metrics.f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
       "              learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
       "              min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
       "              nthread=None, objective='binary:logistic', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "              silent=None, subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb.XGBClassifier()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "xg_clf = xgb.XGBClassifier(objective ='binary:logistic', \n",
    "                           colsample_bytree = 0.5, \n",
    "                           subsample = 0.5,\n",
    "                           learning_rate = 0.1,\n",
    "                           max_depth = 3, \n",
    "                           alpha = 1, \n",
    "                           #scale_pos_weight= titanic['Survived'].mean(),\n",
    "                           n_estimators = 1000,\n",
    "                          verbosity=1,\n",
    "                          random_state=100,\n",
    "                          n_jobs = -1,\n",
    "                          silent=True, \n",
    "                           scale_pos_weight = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(alpha=1, base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=0.5, gamma=0,\n",
       "              learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
       "              min_child_weight=1, missing=None, n_estimators=1000, n_jobs=-1,\n",
       "              nthread=None, objective='binary:logistic', random_state=100,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=20, seed=None,\n",
       "              silent=True, subsample=0.5, verbosity=1)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xg_clf.fit(X_train_scale1,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.512748\n",
      "F1: 0.425450\n"
     ]
    }
   ],
   "source": [
    "preds = xg_clf.predict(X_test_scale1)\n",
    "\n",
    "\n",
    "test_f1 = metrics.f1_score(y_test, preds)\n",
    "test_acc = metrics.accuracy_score(y_test, preds)\n",
    "\n",
    "print(\"Accuracy: %f\" % (test_acc))\n",
    "print(\"F1: %f\" % (test_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refitting the best model to the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_final = RandomForestClassifier(class_weight= {0: 20, 1: 80}, \n",
    "                             criterion='gini', \n",
    "                             max_depth= 3, \n",
    "                             oob_score=True,\n",
    "                             min_samples_leaf= 6, \n",
    "                             random_state=100, \n",
    "                              n_estimators=150,\n",
    "                             verbose=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 150 out of 150 | elapsed:    1.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test F1 score:  0.518290808484476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 150 out of 150 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "rfc_final.fit(X_train_scale1, y_train)\n",
    "\n",
    "#use the fitted model to predict on the test data\n",
    "rfc_preds = rfc_final.predict(X_test_scale1)\n",
    "\n",
    "rfc_f1 = metrics.f1_score(y_test, rfc_preds)\n",
    "\n",
    "# checking accuracy on the test data\n",
    "print('Test F1 score: ', rfc_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final model saved using pickle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "#creating model.pickle file\n",
    "pickle_out = open(\"model.pickle\",\"wb\")\n",
    "pickle.dump(rfc_final, pickle_out) #fill model name here\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
